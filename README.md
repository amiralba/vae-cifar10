# ðŸ§  Variational Autoencoder (VAE) on CIFAR-10

This project implements a **Variational Autoencoder (VAE)** from scratch using PyTorch and trains it on the **CIFAR-10** dataset. The goal is to learn compressed, meaningful representations of images in a continuous latent space, and reconstruct input images from sampled latent vectors.

---

## ðŸ§¾ Key Features

- âœ… Custom-built VAE using `Conv2d`, `Linear`, `ConvTranspose2d`
- âœ… Reparameterization trick for backprop-friendly latent sampling
- âœ… Combined loss: **Reconstruction (MSE) + KL Divergence**
- âœ… Logs and visualizes original vs reconstructed images
- âœ… Built in Google Colab for easy experimentation

---

## ðŸ§ª What I learned

- How VAEs differ from traditional autoencoders  
- How to implement probabilistic encodings with `mu` and `log_var`  
- How the **KL divergence** loss organizes latent space  
- How to visualize reconstructions from a generative model
---
Amir Tabatabaei
ML/AI Engineer | Deep Learning & Computer Vision
LinkedIn
GitHub
